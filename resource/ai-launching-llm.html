<!DOCTYPE html>
<html lang="en-GB" class="noJS" itemscope itemtype="http://schema.org/Article">
<head>
<!-- This website is written by a guy who claims to have lots of specialised technical skills, but this website only partially demonstrates them.  This website is a vehicle for about 200,000 words, please read some of them. -->
<title>Intro for LLM</title>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="en-GB" />
<meta name="Author" content="Owen Beresford" />
<meta name="Description" content="A comprehensive list of algorithms for LLM based AI and related technologies. Also includes definitions of each terms for AI context, and 19 tuning algorithms to improve LLM." />
<meta name="google-site-verification" content="lSgIe7Nm0H0RYQ2ktQ4vr5Jz0iYGhQd7cTWoVDg3Xss" />
<link href="/asset/favicon-32x32.png" rel="icon" type="image/png" />
<meta itemprop="name" content="Intro for LLM">
<meta itemprop="description" content="A comprehensive list of algorithms for LLM based AI and related technologies. Also includes definitions of each terms for AI context, and 19 tuning algorithms to improve LLM.">
<meta name="twitter:site" content="@channelOwen">
<meta name="twitter:title" content="Intro for LLM">
<meta name="twitter:description" content="A comprehensive list of algorithms for LLM based AI and related technologies. Also includes definitions of each terms for AI context, and 19 tuning algorithms to improve LLM.">
<meta name="twitter:creator" content="@channelOwen">
<meta property="og:title" content="Intro for LLM" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://owenberesford.me.uk/resource/ai-launching-llm" />
<meta property="og:description" content="A comprehensive list of algorithms for LLM based AI and related technologies. Also includes definitions of each terms for AI context, and 19 tuning algorithms to improve LLM." />
<meta property="og:site_name" content="OwenBeresford's very wordy site" />
<meta property="article:published_time" content="20th of Sep 2024, 22:24:22" />
<meta property="article:modified_time" content="20th of Sep 2024" />
<link rel="canonical" href="https://owenberesford.me.uk/resource/ai-launching-llm" />
<!-- the below track is just a generic cheese track, but the style fits. progressive + uplifting tone.  I do not own the rights or anything. 
TODO: magic tune selection against some index/DB -->
<meta property="og:audio" content="https://www.youtube.com/watch?v=Brl7WmHDG-E" />

<link rel="stylesheet" href="/asset/ob1.min.css" />
<script type="application/ld+json">
  {
    "@context": "https://ogp.me/ns/article#",
    "@type": "Article",
    "name": "Intro for LLM",
	"article:published_time":"20th of Sep 2024, 22:24:22", 
    "article:modified_time":"20th of Sep 2024",
    "article:section":"technology",

    "author": {
      "@type": "Person",
      "name": "Owen Beresford"
    }
  }
</script>
</head>
<body id="body" class="annoyingBody">
 <div class="h4_page wholeArticle">
  <div class="after_menu articleContent">
   <main id="main">
    <article>
     <div class="blocker addReferences">
<div class="halferWords">
<p class="buttonBar"> 
<span href="/resource/ai-launching-llm" class="button disabled" title="This article. An article looking at the LLM algorithms and related software bits." >LLM concepts </span>
<a href="/resource/ai-retrieval-augmented-generation" class="button" title="This article. An article looking at the RAG extension for LLM, its best practices and common implementations.">RAG Notes</a>
<a href="/resource/ai-vector-stores" class="button" title="An article examining the specialised storage needed for LLM data.">Vector stores </a>
<a href="/resource/ai-testing" class="button" title="LLM are evolved, unlike other software.  However they still need testing.">AI testing </a>
<a href="/resource/ai-tune-llm" class="button" title="A detailed list of actions that that deliver better LLM based products.">Tuning LLM </a>
</p>
</div>
<div class="halferWords">

<h3 class="dontend" id="toc0"> <a href="#toc0" title="Jump to this section." > Can you spot AI output? <sup><i class="fa fa-link invert" aria-label="Jump this to this section." aria-hidden="true"></i></sup> </a></h3>
<p>The current edition of AI / LLM can be used to build web presence and strengthen brands.  However they make distinctive output, limiting to English in this article.  So if you are not using AI / LLM <sup><a href="https://arstechnica.com/ai/2024/07/the-telltale-words-that-could-identify-generative-ai-text/" target="_blank">1</a></sup> <sup><a href="https://fia.umd.edu/using-llms-to-find-amazing-words-that-fit-a-pattern-or-chatgpt-bard-and-cottagecore/" target="_blank">2</a></sup>, it is advisable to avoid:</p>

<ul class="ulbasic">
    <li>“Delve”</li>
    <li>“Explore”</li>
    <li>“Tapestry” </li>
    <li>“Testament”  </li>
    <li>“Leverage”</li>
    <li>“Showcasing”  </li>
    <li>“Underscores” </li>
    <li>“Potential” +4.1% </li>
    <li>“Findings” + 2.7%  </li>
    <li>“Crucial” + 2.6% </li>
</ul>

<p>For some clear diagrams on the basic LLM activity flow, see <sup><a href="https://blog.langchain.dev/tutorial-chatgpt-over-your-data/" target="_blank">3</a></sup>.  It would be better if the diagram included RAG, as a parallel option to the VectorDB.</p>


<h3 class="dontend" id="toc1"> <a href="#toc1" title="Jump to this section." > Structures <sup><i class="fa fa-link invert" aria-label="Jump this to this section." aria-hidden="true"></i></sup> </a></h3>
<p>I list many specific terms here, to make the rest of the group of articles easier to read.   For words to be <b>useful</b>, they should be orthogonal and well organised.   AI currently is so very much more of an <strong>advertisement space</strong> than other tech <sup><a href="https://www.techpolicy.press/challenging-the-myths-of-generative-ai/" target="_blank">4</a></sup>, and access to terms helps structure and deflate 3rd party data.  Note, much of this is stats or words to describe stats.  Browsing the terminology shows what LLM authors needed to describe, and so how LLM are built.</p>

<p>This group of articles is mostly focused on how to build and manage a new LLM, but I think <em>some</em> theory will make it easier to understand the terminology and structures.   There is frequent mention to Gradients in ML, Neural nets, and now LLM, this is the view of the rate of change towards the desired data.  I have simplified *massively* that sentence.   A better and more detailed view is the following: first the pure Maths <sup><a href="https://en.wikipedia.org/wiki/Vector_calculus" target="_blank">5</a></sup> <sup><a href="https://en.wikipedia.org/wiki/Gradient" target="_blank">6</a></sup>, then application theories <sup><a href="https://machinelearningmastery.com/gradient-in-machine-learning/" target="_blank">7</a></sup> <sup><a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" target="_blank">8</a></sup>.  I mention Rate of change in various places to describe curves <sup><a href="https://en.wikipedia.org/wiki/Gallery_of_curves" target="_blank">9</a></sup> <sup><a href="https://en.wikipedia.org/wiki/Curve" target="_blank">10</a></sup>.  According to the last Wiki, I should swap the word 'linear' (constant amount of change in one axis value leads to a constant amount of change in the other axis value)  for 'line' <sup><a href="https://en.wikipedia.org/wiki/Line_%28geometry%29" target="_blank">11</a></sup>, but I think that is ambiguous, as all the graphs have lines.  I also use Stochastic <sup><a href="https://en.wikipedia.org/wiki/Stochastic" target="_blank">12</a></sup></p>

<p>Some people publish articles of their org's internal process, eg “hardware considerations and budget”.  It is very obvious that all software needs to run on some hardware, and better hardware will give results faster than cheaper / older / slower hardware, this is not an AI term and shouldn't be listed.</p>


</div>
<div class="quiteWide">
<dl>
    <dt><strong>LLM (“Large Language Model”)</strong></dt>
        <dd>[algorithm] The name of the algorithm and process to make the current attempt at AI.  In a self-explanatory fashion, it uses large amount of data and forms a language model.</dd>
    <dt><strong>RAG (“Retrieval-Augmented Generation”)</strong></dt>
        <dd>[algorithm] Defined in <sup><a href="https://arxiv.org/abs/2005.11401" target="_blank">1</a></sup>, where LLM created data is enhanced with plain data derived from search <sup><a href="https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/" target="_blank">2</a></sup> <sup><a href="https://research.ibm.com/blog/retrieval-augmented-generation-RAG" target="_blank">3</a></sup>.</dd>
    <dt><strong>NLP, (“natural language programming”)</strong></dt>
        <dd>[algorithm] ~ not part of AI ~ it refers to algorithms to extract the meaning of what a human meant by a stream of text <sup><a href="https://arxiv.org/abs/2212.05773" target="_blank">4</a></sup> <sup><a href="https://arxiv.org/abs/2307.02503" target="_blank">5</a></sup>.  There are quite a few NLP packages in Python.</dd>
    <dt><strong>“Transformers”</strong></dt>
        <dd>[algorithm] Neural architecture that forms the basis of most LLMs <sup><a href="https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29" target="_blank">6</a></sup>.  Made from layers of their neural networks, just like Tensorflow.</dd>
    <dt><strong>“Chain of Note”</strong></dt>
        <dd>[algorithm] A further extension to RAG, aimed at improving results <sup><a href="https://www.rungalileo.io/blog/mastering-rag-llm-prompting-techniques-for-reducing-hallucinations" target="_blank">7</a></sup> <sup><a href="https://arxiv.org/abs/2311.09210" target="_blank">8</a></sup></dd>
    <dt><strong>“Gradient Clipping”</strong></dt>
        <dd>[algorithm] The rate that a LLM correlates with training data, this a scalar magnitude. This prevents “exploding gradients and stabilizes the training process” <sup><a href="https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971" target="_blank">9</a></sup>.  I <em>think</em> this is quadratic or higher power.  Term used for Back propagation.</dd>
    <dt><strong>“Back propagation”</strong></dt>
        <dd>[algorithm] An algorithm from ML that guesses gradients <sup><a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank">10</a></sup>, this is used to make Neural nets work.  Often used with Gradient descent, and it's sometimes called “stochastic gradient descent”.</dd>
    <dt><strong>“Stop Sequences”</strong></dt>
        <dd>[algorithm] Roughly an algorithm, it defines the desired last token in the output.  It is likely to either be a string of &quot;.&quot; OR an enum value 2 which specifies a paragraph.   Unsure about scope for this item, but it's found in <sup><a href="https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/" target="_blank">11</a></sup>.</dd>
    <dt><strong>“Attention”</strong></dt>
        <dd>[algorithm] A weighting of a token compared to the others in the prompt.  This helps make better responses, and first used in <sup><a href="https://arxiv.org/abs/1706.03762" target="_blank">12</a></sup>.  Mostly used to setup RAG.  Note the structural similarity with Google's backlinks algorithm.</dd>
    <dt><strong>“Rerank”, “Reranking”</strong></dt>
        <dd>[algorithm] Resorting your current Resultset to match fresh data from a RAG <sup><a href="https://medium.com/llamaindex-blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6" target="_blank">13</a></sup>.</dd>
    <dt><strong>“Vector store”</strong>, or <strong>&quot;VectorDB”</strong></dt>
        <dd>[service] Software to <a href="https://owenberesford.me.uk/resource/ai-vector-stores#">store</a> and to retrieve vectors at scale.  These are unstructured data heaps, so although databases can be used, its performance needs to be measured.</dd>
    <dt><strong>“Learning Rate Scheduling”</strong></dt>
        <dd>[algorithm] Self-explanatory, a metric for reducing learning rate.  Can be linear or exponential decay.</dd>
    <dt><strong>“Epochs”</strong></dt>
        <dd>[query option] Are also known as “training iterations”, similar to previous ML and tensor flow workflows <sup><a href="https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971" target="_blank">14</a></sup>.  As iterative computation, to translate the entire source data, to extract meaning.</dd>
    <dt><strong>“Temperature”</strong></dt>
        <dd>[query option] A setting for an LLM, that sets how tightly clustered the selected response data is (sets the width / distribution of a stats bell curve) <sup><a href="https://ai.stackexchange.com/questions/32477/what-is-the-temperature-in-the-gpt-models" target="_blank">15</a></sup>.</dd>
    <dt><strong>“Frequency”</strong> penalty</dt>
        <dd>[query option] A setting to avoid a particular Vector repeating in results, by requesting a diverse vocab <sup><a href="https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/" target="_blank">16</a></sup>.</dd>
    <dt>“Regularization”</dt>
        <dd>[query option] Common process like 'dropout' <sup><a href="https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275" target="_blank">17</a></sup> or 'weight decay' <sup><a href="https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/" target="_blank">18</a></sup> <sup><a href="https://ar5iv.labs.arxiv.org/html/2310.04415" target="_blank">19</a></sup>.</dd>
    <dt><strong>“Presence”</strong> penalty</dt>
        <dd>[query option] A setting this mechanically reduces duplication of a Vector.</dd>
    <dt><strong>“Top-K” sampling</strong></dt>
        <dd>[query option] A filter applied to vectors to constrain results a linear minimum value of K <sup><a href="https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/" target="_blank">20</a></sup>.  Filtering is on discrete probability of the next Vector, from the population.</dd>
    <dt><strong>“Top-P” sampling</strong></dt>
        <dd>[query option] A filter applied to the sum of K values to finish the sample, see Top-K.  There will be multiple possible matches.</dd>
    <dt><strong>“Batch”</strong> (size/ count)</dt>
        <dd>[query option] A param to manage over-fitting and under-fitting flaws <sup><a href="https://medium.com/@mccartni/implications-of-batch-size-on-llm-training-and-inference-3320cb48d610" target="_blank">21</a></sup>.  Smaller batch sizes generate more stochastic updates, and larger batch sizes supply more useful generalization <sup><a href="https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971" target="_blank">22</a></sup>.</dd>
    <dt><strong>“Learning Rate”</strong></dt>
        <dd>[query option] Which stats to use to keep the 'shape' of the current training data.  Common options are Time-based decay, Step decay, Exponential decay.</dd>
    <dt><strong>“Hallucination”</strong></dt>
        <dd>[description] Technical people advise not to use this term, as it's poor modelling of the software.  However it's common usage.  Some results of a LLM are just a <i>probable answer</i> <sup><a href="https://www.rungalileo.io/blog/mastering-rag-llm-prompting-techniques-for-reducing-hallucinations" target="_blank">23</a></sup>, and have no basis in reality.</dd>
    <dt><strong>“Model”</strong></dt>
        <dd>[description] Models are the output of an algorithm that has been applied to a dataset <sup><a href="https://www.ibm.com/topics/ai-model" target="_blank">24</a></sup>.</dd>
    <dt><strong>“Token”</strong></dt>
        <dd>[description] The smallest section from the prompt that makes sense <sup><a href="https://medium.com/@cloudswarup/the-building-blocks-of-llms-vectors-tokens-and-embeddings-1cd61cd20e35" target="_blank">25</a></sup> ~ often a whole word ~ but not always e.g. “could”, vs [“could”, “n't”].  Cohere ltd uses 4 bytes per token, on average <sup><a href="https://cohere.com/blog/llm-parameters-best-outputs-language-ai" target="_blank">26</a></sup> ~ this would be 1 big5 ideogram (traditional Chinese), or a short word in en-US.   A token is used as “converted into a vector via looking up from a word embedding table” <sup><a href="https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29" target="_blank">27</a></sup>.</dd>
    <dt><strong>“Vector”</strong></dt>
        <dd>[description] A vector is a float, or list of floats when used in an AI context.  Vectors have a real number range.   A prompt and the original data population are converted into vectors to make handing easier.  This <i>should</i> apply to speech, video and text.  For an idea of the scale of data: 2 short 4-word-sentences map to 1,500 vectors with recent Python libraries.</dd>
    <dt><strong>“Embeddings”</strong></dt>
        <dd>[description] A larger collection of vectors.   Embeddings are useful vectors <sup><a href="https://byronsalty.medium.com/my-growing-list-of-ai-and-llm-terminology-26d8b109a14f" target="_blank">28</a></sup>.  An article used the term “trained” data.</dd>
    <dt><strong>“Parametric” / “parameterised”</strong></dt>
        <dd>[description] A statistical model <sup><a href="https://en.wikipedia.org/wiki/Parametric_model" target="_blank">29</a></sup> that records the distribution of probabilities.  Knowing the distribution means better prompt responses can be created <sup><a href="https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/#2-2-parametric-vs-non-parametric-memory" target="_blank">30</a></sup> <sup><a href="https://lagstill.medium.com/rag-analytics-explored-8d389978880f" target="_blank">31</a></sup>.  Not used by itself.</dd>
    <dt><strong>“Context Window”</strong></dt>
        <dd>[description] The data that is carried along with the user query, so the LLM can make better responses. Analogous to a person's short term memory <sup><a href="https://medium.com/@crskilpatrick807/context-windows-the-short-term-memory-of-large-language-models-ab878fc6f9b5" target="_blank">32</a></sup>.  Sizes of window for some common AI <sup><a href="https://swimm.io/learn/large-language-models/llm-context-windows-basics-examples-and-prompting-best-practices" target="_blank">33</a></sup></dd>
    <dt><strong>“Early Stopping”</strong></dt>
        <dd>[description] Not using vast numbers of Epochs, to reduce Overfitting.  See nice graphs in here <sup><a href="https://towardsai.net/p/data-science/pause-for-performance-the-guide-to-using-early-stopping-in-ml-and-dl-model-training" target="_blank">34</a></sup>, term also used in Tensorflow.</dd>
    <dt><strong>“Overfit”</strong></dt>
        <dd>[description] Applying the control levers on an Epoch too tightly (see learning rate), so later responses very tightly match training data.   Examples <sup><a href="https://betterprogramming.pub/large-language-model-knowledge-graph-store-yes-by-fine-tuning-llm-with-kg-f88b556959e6" target="_blank">35</a></sup>.   This limits usefulness of the LLM, as noise in the original data is outputted.</dd>
    <dt><strong>“Underfit”</strong></dt>
        <dd>[description] When the LLM isn't learning from a training sample, the antonym is above 'overfit'.</dd>
    <dt><strong>“Fine-Tuning”</strong></dt>
        <dd>[description] Refers to “small changes” rather than big ones <sup><a href="https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29" target="_blank">36</a></sup>, computationally this is a smaller change.  Not sure on boundary between large tuning and fine tuning.</dd>
    <dt><strong>One-Shot</strong></dt>
        <dd>[description] A style of prompting, to steer the results <sup><a href="https://www.whytryai.com/p/zero-shot-one-shot-few-shot-prompting" target="_blank">37</a></sup></dd>
    <dt>“Max Output Tokens”</dt>
        <dd>A limit to the size of the generated output.</dd>
    <dt><strong>Weights</strong></dt>
        <dd>A standard stats concept that attaches a relative importance to each value in a list, so the most important one has most impact on the result computation.</dd>
    <dt><strong>Quantization</strong></dt>
        <dd>Arts people would say posterisation, quantisation is reducing the range of possible values in a uniform fashion. ..maybe 4bit / 16 value..</dd>
    <dt><strong>“AGI”</strong> or <strong>“Artificial General Intelligence”</strong></dt>
        <dd>A distant ambition that we currently do not have even a whisper of.   Marketing people sometimes get excited and claim this already.</dd>
    <dt>Metacognition</dt>
        <dd>Maybe in some distant Star trek themed future...</dd>
    <dt>GPT (Generative Pre-trained Transformer)</dt>
        <dd>i.e. the technical end of chatGPT, the rest of this document is the explanation.</dd>
    <dt><strong>“Prompt”</strong></dt>
        <dd>[input] The question, the request, the triggering event.  Normally the human-entered text, but should include vocal samples.</dd>
    <dt>✨✨</dt>
        <dd><strong>✨✨  The following is a batch of tuning algorithms ✨✨ </strong></dd>
    <dt><strong>RLHF, (“Reinforcement Learning with Human Feedback”)</strong></dt>
        <dd>[algorithm] Used in ML, deep learning, LLM and now RAG.  Basically it adds a feedback path so less good answers can be made less common.</dd>
    <dt><strong>BPE (“Byte-Pair Encoding”)</strong></dt>
        <dd>[algorithm] A high value algorithm, which started out to compress data <sup><a href="https://datascience.fm/understanding-byte-pair-encoding-bpe-in-large-language-models-llms/" target="_blank">38</a></sup>.  This simplifies neural nets, and supports extending with fresh data.</dd>
    <dt><strong> KD (“Knowledge Distillation”)</strong></dt>
        <dd>[algorithm] A technique in which a large language model transfers its knowledge <sup><a href="https://arxiv.org/pdf/2306.08543v1" target="_blank">39</a></sup> to a smaller model to achieve similar performance with reduced computational resources.</dd>
    <dt><strong>“Transfer Learning”</strong></dt>
        <dd>[algorithm] Borrowing the config from a large dataset to your smaller one.</dd>
    <dt><strong>“Knowledge graph”</strong></dt>
        <dd>[algorithm] A method for storing data and known facts.  The Graph should increase the rate that LLM make useful answers, rather than answers <sup><a href="https://betterprogramming.pub/large-language-model-knowledge-graph-store-yes-by-fine-tuning-llm-with-kg-f88b556959e6" target="_blank">40</a></sup>.  Article lists several different GraphDB syntaxes.</dd>
    <dt><strong>&quot;GloVe” (Global Vectors for Word Representation)</strong></dt>
        <dd>[algorithm] An unsupervised learning algorithm for obtaining vector representations for words <sup><a href="https://nlp.stanford.edu/projects/glove/" target="_blank">41</a></sup> ~ used for NLP.</dd>
    <dt><strong>RNN (“Recurrent Neural Networks”)</strong></dt>
        <dd>[algorithm] A neural network with weights and an internal state <sup><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank">42</a></sup>.</dd>
    <dt><strong>CNN (“Convolutional Neural Networks”)</strong></dt>
        <dd>[algorithm] A regularized type of feed-forward neural network that learns features by itself <sup><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_blank">43</a></sup>.   Also known as 'Shift Invariant' or 'Space Invariant artificial Neural networks'.</dd>
    <dt><strong>LSTM (Long-Short-Term-Memory)</strong> network</dt>
        <dd>[algorithm] A type of RNN aimed at dealing with the vanishing gradient problem present in other RNN <sup><a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank">44</a></sup>.</dd>
    <dt><strong>PEFT (Parameter-Efficient Fine-Tuning)</strong></dt>
        <dd>[algorithm] A label for a group of methods to improve performance in LLM with training <sup><a href="https://www.ibm.com/think/topics/parameter-efficient-fine-tuning" target="_blank">45</a></sup>.  See also 'Prompt Tuning', 'LoRA (Low-Rank Adaptation)', and 'Prefix Tuning', also listed.</dd>
    <dt><strong>LoRA (Low Rank Adaptation)</strong></dt>
        <dd>[algorithm] Applies small alterations to the cross-attention layers of Stable Diffusion models <sup><a href="https://wiki.civitai.com/wiki/Low-Rank_Adaptation" target="_blank">46</a></sup>.  However, not big enough to have its own wiki page <sup><a href="https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29#Low-rank_adaptation" target="_blank">47</a></sup>.   See also QLoRA or Quantised LoRA.</dd>
    <dt><strong>“Prefix tuning”</strong></dt>
        <dd>[algorithm] Created for natural language generation (NLG), this prepends a task-specific vector.</dd>
    <dt><strong>“Prompt tuning”</strong></dt>
        <dd>[algorithm] Injects tailored prompts into the input or training data to improve responses.  Soft prompts, generated by the AI from known data have the best accomplishments.</dd>
    <dt><strong>BERT (“Bidirectional Encoder Representations from Transformers”)</strong></dt>
        <dd>[algorithm] Invented by Google Inc, BERT is an “encoder-only” Transformer architecture <sup><a href="https://en.wikipedia.org/wiki/BERT_%28language_model%29" target="_blank">48</a></sup>.  There are various sizes of data set which will support different LLM outcomes.</dd>
    <dt><strong>MoE (“Mixture of Experts”)</strong></dt>
        <dd>[algorithm] Splits a large subject area into smaller “experts”, each prompt will probably only be answered by one expert <sup><a href="https://en.wikipedia.org/wiki/Mixture_of_experts" target="_blank">49</a></sup>.  Wiki has a maths description.</dd>
    <dt><strong>STP (“Step-back Prompting”)</strong></dt>
        <dd>[algorithm] Gets the platform to edit the prompt to a higher level abstract query to “derive high-level concepts and first principle” <sup><a href="https://blog.svenson.ai/step-back-prompting-a-new-technique-for-abstraction-and-reasoning-in-large-language-models" target="_blank">50</a></sup>.</dd>
    <dt><strong>RoPE (“Rotary Positional Embedding”)</strong></dt>
        <dd>[algorithm] Whilst doing Vector generation, RoPE remembers the position of a word in a sentence <sup><a href="https://medium.com/ai-insights-cobet/rotary-positional-embeddings-a-detailed-look-and-comprehensive-understanding-4ff66a874d83" target="_blank">51</a></sup>, this reference offers several algorithms for the offset, suited to different sizes of prompt text.</dd>
    <dt><strong>ULMFiT (“Universal Language Model Fine-Tuning”)</strong></dt>
        <dd>[algorithm] Dedicated NLP algorithm, Quote “It involves a 3-layer AWD-LSTM architecture for its representations. The training consists of three steps:</dd>
</dl>

<ol>
    <li>general language model pre-training on a Wikipedia-based text, </li>
    <li>fine-tuning the language model on a target task, and </li>
    <li>fine-tuning the classifier on the target task.&quot; <sup><a href="https://paperswithcode.com/method/ulmfit" target="_blank">52</a></sup></li>
</ol>

<dl>
    <dt><strong>XLNet</strong></dt>
        <dd>[algorithm] Complicated, please read reference <sup><a href="https://paperswithcode.com/method/xlnet" target="_blank">53</a></sup></dd>
</dl>

<p>A view to the future <sup><a href="https://epochai.org/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data" target="_blank">54</a></sup> <sup><a href="https://arxiv.org/pdf/2211.04325" target="_blank">55</a></sup>.   A nice article, where people are addicted to stats <sup><a href="https://www.fast.ai/posts/2023-09-04-learning-jumps/" target="_blank">56</a></sup>.   Some general 'SEO articles' selected as comparative to this article <sup><a href="https://www.coursera.org/collections/llms-terms" target="_blank">57</a></sup> <sup><a href="https://byronsalty.medium.com/my-growing-list-of-ai-and-llm-terminology-26d8b109a14f" target="_blank">58</a></sup> <sup><a href="https://toloka.ai/blog/history-of-llms/" target="_blank">59</a></sup>.   An example of LLM gone bad <sup><a href="https://www.meetgrit.com/s/articles/a06Qp00000C09dtIAB/a-compact-guide-to-large-language-models-defining-llms-and-their-role-in-ai" target="_blank">60</a></sup> <sup><a href="https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971" target="_blank">61</a></sup>.  If you have a medium account, try <sup><a href="https://medium.com/@msayef/llm-terminologies-decoded-a-beginners-quick-reference-guide-925f35794198" target="_blank">62</a></sup> <sup><a href="https://medium.com/tales-of-tomorrow/a-quiet-revolution-mistral-ai-releases-sensational-new-ai-model-c17c663287f0" target="_blank">63</a></sup> <sup><a href="https://towardsdatascience.com/gpt-3-rnns-and-all-that-deep-dive-into-language-modelling-7f67658ba0d5" target="_blank">64</a></sup> <sup><a href="https://ai.plainenglish.io/fine-tuning-large-language-models-for-decision-support-a-comprehensive-guide-c1ef4c06abc6" target="_blank">65</a></sup>.</p>


</div>
</div>
    </article>
   </main>
	<div id="contentGroup" class="adjacentGroup" data-group="engineering, english" title="Use the first link to get the complete range of the group." > <p>Some similar articles in engineering </p>
<div id="groupengineering" class="adjacentList"><a class="adjacentItem button" href="/resource/group-XXX?first=engineering" aria-label="This article lists all items in engineering group.">All of <br />engineering<br /> articles </a> <noscript>Seeing this means the Adjacent feature is <strong>disabled</strong><br /> Try the full page link on the left </noscript></div>
<p>Some similar articles in english </p>
<div id="groupenglish" class="adjacentList"><a class="adjacentItem button" href="/resource/group-XXX?first=english" aria-label="This article lists all items in english group.">All of <br />english<br /> articles </a> <noscript>Seeing this means the Adjacent feature is <strong>disabled</strong><br /> Try the full page link on the left </noscript></div>
 </div>

  </div>
  <fieldset class="outer_menu articleHeader">
	<legend></legend>
	<nav>
		<div id="navBar" class="row">
			<div class="column">
				<div class="top-bar fullWidth">
					<header><h1>Launching my study of AI/LLM</h1></header>
			    	<p role="status" class="bigScreenOnly">  *I typed this*, but a prompt of "Return every algorithm and every keyterm to make a good LLM" would be similar.  </p>
				</div>
				<div id="shareGroup" class="bibbles row addReading">
					<span class="allButtons"> 
						<a id="siteChartLink" class="button smallScreenOnly" href="/resource/site-chart" title="open a webpage of what articles this site holds.">Sitemap</a>
						<a id="rssLink" href="https://owenberesford.me.uk/resource/rss" title="Access the sites RSS feed."> <i class="fa fa-rss" aria-label="Open the RSS for this site." aria-hidden="true"></i><span class="sr-only">RSS</span></a> 
						<span class="button smallScreenOnly" id="shareMenuTrigger" rel="nofollow" aria-haspopup="menu" > Share </span>
						<span class="bigScreenOnly">Share: </span>
                        <a href="https://twitter.com/intent/tweet?text=I+think+this+is+important+https%3A%2F%2Fowenberesford.me.uk%2Fresource%2Fai-launching-llm" title="Share this resource on your twitter account." target="_blank" class="bigScreenOnly"> <i class="fa fa-twitter" aria-label="Share this resource on your twitter account." aria-hidden="true"><span class="sr-only">Twitter</span> </i></a>
						<a href="#" id="mastoTrigger" class="masto bigScreenOnly" title="Share this article with *your* mastodon instance" aria-haspopup="dialog" >	<i class="fa fa-mastodon" aria-label="Share this article on *your* mastodon instance." aria-hidden="true"></i><span class="sr-only">Mastodon</span> </a>

						<a href="https://www.reddit.com/submit?url=https%3A%2F%2Fowenberesford.me.uk%2Fresource%2Fai-launching-llm" target="_blank" title="Share this article with your Reddit audience" class="bigScreenOnly" ><i aria-label="Share this article with your Reddit audience." class="fa fa-reddit-square" aria-hidden="true"></i><span class="sr-only">Reddit </span> </a>
						<a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fowenberesford.me.uk%2Fresource%2Fai-launching-llm" target="_blank" class="bigScreenOnly" title="Share current article with your linked-in audience." ><i class="fa fa-linkedin-square" aria-hidden="true" aria-label="Share this article with your linked-in audience."></i><span class="sr-only">Linkedin</span> </a>
						<a title="Share current article with Hacker news/ Y combinator audience" target="_blank" class="bigScreenOnly" href="http://news.ycombinator.com/submitlink?u=https%3A%2F%2Fowenberesford.me.uk%2Fresource%2Fai-launching-llm&amp;t=Intro+for+LLM"> <i class="fa fa-hacker-news" aria-label="Share this article with your Y combinator audience." aria-hidden="true"> </i><span class="sr-only">Hacker new</span> </a>

						<a title="Share this article with your Xing audience." href="https://www.xing.com/spi/shares/new?url=https%3A%2F%2Fowenberesford.me.uk%2Fresource%2Fai-launching-llm" target="_blank" class="bigScreenOnly" ><i class="fa fa-xing-square" aria-hidden="true" aria-label="Share this article with your Xing audience."></i><span class="sr-only">Xing</span> </a>
					</span>

					<span class="ultraSkinny bigScreenOnly"> 
						<span>Edited <time datetime="2024-09-20T19:56:03">20th of Sep 2024</time>
						</span>
						<span>Created <time datetime="2024-09-10T00:00:00" title="If the value says 03-03-2015; its wrong but that is when this project was moved to the current git project" >10th of Sep 2024</time> </span>
					</span>

				</div>
			</div>
			<dialog id="popup" class="popup1 bigScreenOnly">
				<form method="dialog" encoding="multipart/form-data" action="." name="mastoSelection"  >
					<label for="mastodonserver">your server: 
						<input id="mastodonserver" max-length="50" data-url="https%3A%2F%2Fowenberesford.me.uk%2Fresource%2Fai-launching-llm" type="text" value="" placeholder="mastodon.social" />  
					</label> 
					<span id="sendMasto" class="button masto" title="Share article to *your* mastodon server">Share article now</span>
					<span class="button trimmed" id="hideMasto" title="Close popup"> <i class="fa fa-cancel" aria-hidden="true"></i> Cancel </span>
				</form>
			</dialog>

<fieldset class="h4_menu column bigScreenOnly ">
<legend><span id="pageMenu" aria-haspopup="menu"><i class="fa fa-ob1burger" aria-hidden="true"></i><span class="sr-only">Menu</span> </span></legend>
<menu class="h4_lean">
<li class="h4_odd"><a href="#toc0">Can you spot AI output?</a></li>
<li><a href="#toc1">Structures</a></li>
</menu>
<br />

</fieldset>
	</div>
<menu class="burgerMenu" >
<li class="h4_odd">Additional features</li>
<li class=""><a href="/resource/home"><i class="fa fa-angle-left" aria-hidden="true"></i> Home</a> </li> 
<li class="h4_odd"><a href="/resource/search">Search <i class="fa fa-angle-right" aria-hidden="true"></i></a></li>
<li class=""><a href="/resource/appearance">Appearance <i class="fa fa-angle-right" aria-hidden="true"></i></a></li>
<li class="h4_odd"><a href="/resource/contact-me">Contact me <i class="fa fa-angle-right" aria-hidden="true"></i></a></li>
<li class=""><a href="#contentGroup">Similar articles</a></li>
</menu>
	</nav>
</fieldset>
		</div>
 <br class="blocker" />
 <div id="biblio" style="display:none;">
    <br class="blocker" />
 </div>
 
 <footer>
  <div class="h4_footer"> 
	<div class="leftFooter"> 
		<a href="https://www.plainenglish.co.uk/services.html" target="_blank" title="They, er, don't have a service for >200,000 word sites, so no logo.">Campaign for Plain English</a><br />
		My profile: <a href="https://www.linkedin.com/in/owen-beresford-bb6ab030/" target="_blank" aria-label="my linked-in" title="Load my linked-in profile" ><i class="fixLinkedSq fa fa-linkedin-square" aria-hidden="true" aria-label="Open my linked in profile" ></i><span class="sr-only">linkedin</span></a>  
	</div> 
	<p> Page rendered <time datetime="2024-09-20T22:24:22">20th of Sep 2024, 22:24:22</time>, Copyright &copy; 2022 Owen Beresford, <a href="https://owenberesford.me.uk/resource/contact-me">contact me</a>.  Last modified <time datetime="2024-09-20T19:56:03">20th of Sep 2024</time>.
    <p>Read the generous <a rel="license" href="https://owenberesford.me.uk/resource/licence" title="Load the license term; but not that interesting">licence terms</a>, if you feel the need, there is a <a href="https://owenberesford.me.uk/resource/privacy#" title="Load the privacy terms" >privacy here</a>.    View the <a href="https://owenberesford.me.uk/resource/site-chart#" title="Load a page showing all the articles on this site">site map</a>.  <a href="#pageMenu">Jump to menu</a>
</div>
</footer>
<script type="module" src="/asset/ob1-202406.min.mjs" ></script>
<style>
.halferWords ul { margin-left:30%; }

ol { counter-reset: points 0; } 
@media screen and (min-width:800px) {
	ol { margin-left:30%; } 
}
ol li::before { counter-increment: points 1;  content:counter(points) ")";  }

/* custom styling, as this list is too hardcore for the general framework. */
@media screen and (min-width:1024px ) {
	.quiteWide { width:70em; margin-left:auto; margin-right:auto; height:100vh; overflow-y:scroll; }	
	 dl dt { display: inline-block; width: 20%; min-width: 8em; text-align: center; vertical-align: text-top; }
	 dl dd { display: inline-block; max-width: 79%; text-align: left; vertical-align: text-top; min-width: 75%; }
}
@media screen and (max-width:1024px ) and (min-width:600px ) {
	.quiteWide { width:70em; margin-left:auto; margin-right:auto; height:100vh; overflow-y:scroll; }	
	 dl dt { display: inline-block; width: 20%; min-width: 8em; text-align: center; vertical-align: text-top; }
	 dl dd { display: inline-block; max-width: 79%; text-align: left; vertical-align: text-top; min-width: 75%; }
}
@media screen and (max-width:600px ) {
	.quiteWide { width:100vw; height:100vh; overflow-y:scroll; }	
	 dl dt { display: block; min-width: 8em; text-align: center; }
	 dl dd { text-align: left; }
}
</style>
</body>
</html>