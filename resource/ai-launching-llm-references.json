[
  {
    "url": "https://arstechnica.com/ai/2024/07/the-telltale-words-that-could-identify-generative-ai-text/",
    "desc": "New paper counts excess words\" that started appearing more often in the post-LLM era.",
    "title": "The telltale words that could identify generative AI text | Ars Technica",
    "auth": "@KyleOrl / @KyleOrl@mastodon.social",
    "date": 0
  },
  {
    "url": "https://fia.umd.edu/using-llms-to-find-amazing-words-that-fit-a-pattern-or-chatgpt-bard-and-cottagecore/",
    "desc": "Using LLMs to find amazing words that fit a pattern ... or, ChatGPT, Bard, and cottagecore &ndash; FIA",
    "title": "Using LLMs to find amazing words that fit a pattern ... or, ChatGPT, Bard, and cottagecore &ndash; FIA",
    "auth": "Dan Russell",
    "date": 0
  },
  {
    "url": "https://www.techpolicy.press/challenging-the-myths-of-generative-ai/",
    "desc": "Eryk Salvaggio says we must dispense with myths if we are to think more clearly about what AI actually is and does.",
    "title": "Challenging The Myths of Generative AI | TechPolicy.Press",
    "auth": "Eryk Salvaggio",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Vector_calculus",
    "desc": "Vector calculus - Wikipedia",
    "title": "Vector calculus - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Gradient",
    "desc": "Gradient - Wikipedia",
    "title": "Gradient - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://machinelearningmastery.com/gradient-in-machine-learning/",
    "desc": "HTTP_ERROR, Received code 403 code.",
    "title": "",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://machinelearningmastery.com/gradient-descent-for-machine-learning/",
    "desc": "HTTP_ERROR, Received code 403 code.",
    "title": "",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Gallery_of_curves",
    "desc": "Gallery of curves - Wikipedia",
    "title": "Gallery of curves - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Curve",
    "desc": "Curve - Wikipedia",
    "title": "Curve - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Line_%28geometry%29",
    "desc": "Line (geometry) - Wikipedia",
    "title": "Line (geometry) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Stochastic",
    "desc": "Stochastic - Wikipedia",
    "title": "Stochastic - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2005.11401",
    "desc": "[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "title": "[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/",
    "desc": "Retrieval Augmented Generation (RAG) signifies a transformative advancement in large language models (LLMs). It combines the generative prowess of transformer architectures with dynamic information retrieval.  This integration allows LLMs to access a...",
    "title": "Next-Gen Large Language Models: The Retrieval-Augmented Generation (RAG) Handbook",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://research.ibm.com/blog/retrieval-augmented-generation-RAG",
    "desc": "RAG is an AI framework for retrieving facts to ground LLMs on the most accurate information and to give users insight into AI&rsquo;s decision making process.",
    "title": "What is retrieval-augmented generation (RAG)? - IBM Research",
    "auth": "@IBMResearch",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2212.05773",
    "desc": "[2212.05773] A Survey on Natural Language Processing for Programming",
    "title": "[2212.05773] A Survey on Natural Language Processing for Programming",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2307.02503",
    "desc": "[2307.02503] Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review",
    "title": "[2307.02503] Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29",
    "desc": "Transformer (deep learning architecture) - Wikipedia",
    "title": "Transformer (deep learning architecture) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.rungalileo.io/blog/mastering-rag-llm-prompting-techniques-for-reducing-hallucinations",
    "desc": "Dive into our blog for advanced strategies like ThoT, CoN, and CoVe to minimize hallucinations in RAG applications. Explore emotional prompts and ExpertPrompting to enhance LLM performance. Stay ahead in the dynamic RAG landscape with reliable insights for precise language models. Read now for a deep dive into refining LLMs.",
    "title": "Mastering RAG: LLM Prompting Techniques For Reducing Hallucinations",
    "auth": "Pratik Bhavsar",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2311.09210",
    "desc": "[2311.09210] Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
    "title": "[2311.09210] Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Backpropagation",
    "desc": "Backpropagation - Wikipedia",
    "title": "Backpropagation - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/",
    "desc": "A Guide to LLM Hyperparameters | Symbl.ai",
    "title": "A Guide to LLM Hyperparameters | Symbl.ai",
    "auth": "Kartik Talamadupula",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/1706.03762",
    "desc": "[1706.03762] Attention Is All You Need",
    "title": "[1706.03762] Attention Is All You Need",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://medium.com/llamaindex-blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6",
    "desc": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6",
    "title": "using-llms-for-retrieval-and-reranking-23cf2d3a14b6",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://ai.stackexchange.com/questions/32477/what-is-the-temperature-in-the-gpt-models",
    "desc": "machine learning - What is the temperature\" in the GPT models? - Artificial Intelligence Stack Exchange",
    "title": "machine learning - What is the temperature\" in the GPT models? - Artificial Intelligence Stack Exchange",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/",
    "desc": "A Guide to LLM Hyperparameters | Symbl.ai",
    "title": "A Guide to LLM Hyperparameters | Symbl.ai",
    "auth": "Kartik Talamadupula",
    "date": 0
  },
  {
    "url": "https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275",
    "desc": "Understanding Dropout with the Simplified Math behind it",
    "title": "Understanding Dropout with the Simplified Math behind it",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/",
    "desc": "HTTP_ERROR, Received code 403 code.",
    "title": "",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://ar5iv.labs.arxiv.org/html/2310.04415",
    "desc": "[2310.04415] Why Do We Need Weight Decay in Modern Deep Learning?",
    "title": "[2310.04415] Why Do We Need Weight Decay in Modern Deep Learning?",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/",
    "desc": "A Guide to LLM Hyperparameters | Symbl.ai",
    "title": "A Guide to LLM Hyperparameters | Symbl.ai",
    "auth": "Kartik Talamadupula",
    "date": 0
  },
  {
    "url": "https://medium.com/@mccartni/implications-of-batch-size-on-llm-training-and-inference-3320cb48d610",
    "desc": "Implications of Batch Size on LLM training and inference",
    "title": "Implications of Batch Size on LLM training and inference",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://www.rungalileo.io/blog/mastering-rag-llm-prompting-techniques-for-reducing-hallucinations",
    "desc": "Dive into our blog for advanced strategies like ThoT, CoN, and CoVe to minimize hallucinations in RAG applications. Explore emotional prompts and ExpertPrompting to enhance LLM performance. Stay ahead in the dynamic RAG landscape with reliable insights for precise language models. Read now for a deep dive into refining LLMs.",
    "title": "Mastering RAG: LLM Prompting Techniques For Reducing Hallucinations",
    "auth": "Pratik Bhavsar",
    "date": 0
  },
  {
    "url": "https://www.ibm.com/topics/ai-model",
    "desc": "An AI model is a program that applies one or more algorithms to data to recognize patterns, make predictions or make decisions without human intervention.",
    "title": "What Is an AI Model? | IBM",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://medium.com/@cloudswarup/the-building-blocks-of-llms-vectors-tokens-and-embeddings-1cd61cd20e35",
    "desc": "The Building Blocks of LLMs: Vectors, Tokens and Embeddings",
    "title": "The Building Blocks of LLMs: Vectors, Tokens and Embeddings",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://cohere.com/blog/llm-parameters-best-outputs-language-ai",
    "desc": "When using Language AI to generate content, there are many options to control the outputs. Lets take a look at them in this post.",
    "title": "LLM Parameters Demystified: Getting The Best Outputs from Language AI",
    "auth": "@cohere",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29",
    "desc": "Transformer (deep learning architecture) - Wikipedia",
    "title": "Transformer (deep learning architecture) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://byronsalty.medium.com/my-growing-list-of-ai-and-llm-terminology-26d8b109a14f",
    "desc": "Industry Terms",
    "title": "Industry Terms",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Parametric_model",
    "desc": "Parametric model - Wikipedia",
    "title": "Parametric model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/#2-2-parametric-vs-non-parametric-memory",
    "desc": "Retrieval Augmented Generation (RAG) signifies a transformative advancement in large language models (LLMs). It combines the generative prowess of transformer architectures with dynamic information retrieval.  This integration allows LLMs to access a...",
    "title": "Next-Gen Large Language Models: The Retrieval-Augmented Generation (RAG) Handbook",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://lagstill.medium.com/rag-analytics-explored-8d389978880f",
    "desc": "RAG Analytics Explored",
    "title": "RAG Analytics Explored",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://medium.com/@crskilpatrick807/context-windows-the-short-term-memory-of-large-language-models-ab878fc6f9b5",
    "desc": "Context Windows: The Short-term Memory of Large Language Models",
    "title": "Context Windows: The Short-term Memory of Large Language Models",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://swimm.io/learn/large-language-models/llm-context-windows-basics-examples-and-prompting-best-practices",
    "desc": "A context window refers to the amount of text data a language model can consider at one time when generating responses.",
    "title": "LLM Context Windows: Basics, Examples &amp; Prompting Best Practices",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://towardsai.net/p/data-science/pause-for-performance-the-guide-to-using-early-stopping-in-ml-and-dl-model-training",
    "desc": "Author(s): Shivamshinde Originally published on Towards AI. Photo by Aleksandr Kadykov on UnsplashTable of ContentIntroduction- What are Bias and Variance?- ...",
    "title": "Pause for Performance: The Guide to Using Early Stopping in ML and DL Model Training | Towards AI",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://betterprogramming.pub/large-language-model-knowledge-graph-store-yes-by-fine-tuning-llm-with-kg-f88b556959e6",
    "desc": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "title": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29",
    "desc": "Fine-tuning (deep learning) - Wikipedia",
    "title": "Fine-tuning (deep learning) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.whytryai.com/p/zero-shot-one-shot-few-shot-prompting",
    "desc": "Zero-Shot, One-Shot, Few-Shot, More?",
    "title": "Zero-Shot, One-Shot, Few-Shot, More?",
    "auth": "Daniel Nest",
    "date": 0
  },
  {
    "url": "https://datascience.fm/understanding-byte-pair-encoding-bpe-in-large-language-models-llms/",
    "desc": "This fixed vocabulary constraint becomes particularly problematic for languages with complex word formation processes like agglutination and compounding, which can create a vast number of word variations that a fixed-size vocabulary cant cover​​.",
    "title": "Understanding Byte Pair Encoding (BPE) in Large Language Models (LLMs)",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2306.08543v1",
    "desc": "2306.08543v1",
    "title": "2306.08543v1",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://betterprogramming.pub/large-language-model-knowledge-graph-store-yes-by-fine-tuning-llm-with-kg-f88b556959e6",
    "desc": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "title": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://nlp.stanford.edu/projects/glove/",
    "desc": "GloVe: Global Vectors for Word Representation",
    "title": "GloVe: Global Vectors for Word Representation",
    "auth": "Jeffrey Pennington",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "desc": "Recurrent neural network - Wikipedia",
    "title": "Recurrent neural network - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
    "desc": "Convolutional neural network - Wikipedia",
    "title": "Convolutional neural network - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Long_short-term_memory",
    "desc": "Long short-term memory - Wikipedia",
    "title": "Long short-term memory - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.ibm.com/think/topics/parameter-efficient-fine-tuning",
    "desc": "Parameter-efficient fine-tuning (PEFT) is a method of improving the performance of pretrained large language models (LLMs) and neural networks for specific tasks or data sets.",
    "title": "What is parameter-efficient fine-tuning (PEFT)?",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://wiki.civitai.com/wiki/Low-Rank_Adaptation",
    "desc": "Low-Rank Adaptation - Civitai Wiki",
    "title": "Low-Rank Adaptation - Civitai Wiki",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29#Low-rank_adaptation",
    "desc": "Fine-tuning (deep learning) - Wikipedia",
    "title": "Fine-tuning (deep learning) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/BERT_%28language_model%29",
    "desc": "BERT (language model) - Wikipedia",
    "title": "BERT (language model) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Mixture_of_experts",
    "desc": "Mixture of experts - Wikipedia",
    "title": "Mixture of experts - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://blog.svenson.ai/step-back-prompting-a-new-technique-for-abstraction-and-reasoning-in-large-language-models",
    "desc": "Step-Back Prompting is an innovative technique that enables large language models to perform complex reasoning tasks",
    "title": "Step-Back Prompting: A New Technique for Abstraction and Reasoning in Large Language Models",
    "auth": "Svenson.ai",
    "date": 0
  },
  {
    "url": "https://medium.com/ai-insights-cobet/rotary-positional-embeddings-a-detailed-look-and-comprehensive-understanding-4ff66a874d83",
    "desc": "Rotary Positional Embeddings: A Detailed Look and Comprehensive Understanding",
    "title": "Rotary Positional Embeddings: A Detailed Look and Comprehensive Understanding",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://paperswithcode.com/method/ulmfit",
    "desc": "Universal Language Model Fine-tuning, or ULMFiT, is an architecture and transfer learning method that can be applied to NLP tasks. It involves a 3-layer AWD-LSTM architecture for its representations. The training consists of three steps: 1) general language model pre-training on a Wikipedia-based text, 2) fine-tuning the language model on a target task, and 3) fine-tuning the classifier on the target task.\n\nAs different layers capture different types of information, they are fine-tuned to differ",
    "title": "ULMFiT Explained | Papers With Code",
    "auth": "@paperswithcode",
    "date": 0
  },
  {
    "url": "https://paperswithcode.com/method/xlnet",
    "desc": "XLNet is an autoregressive Transformer that leverages the best of both autoregressive language modeling and autoencoding while attempting to avoid their limitations. Instead of using a fixed forward or backward factorization order as in conventional autoregressive models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and ",
    "title": "XLNet Explained | Papers With Code",
    "auth": "@paperswithcode",
    "date": 0
  },
  {
    "url": "https://epochai.org/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data",
    "desc": "We estimate the stock of human-generated public text at around 300 trillion tokens. If trends continue, language models will fully utilize this stock between 2026 and 2032, or even earlier if intensely overtrained.",
    "title": "Will We Run Out of Data to Train Large Language Models?",
    "auth": "Pablo Villalobos",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2211.04325",
    "desc": "2211.04325",
    "title": "2211.04325",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://www.fast.ai/posts/2023-09-04-learning-jumps/",
    "desc": "We&rsquo;ve noticed an unusual training pattern in fine-tuning LLMs. At first we thought it&rsquo;s a bug, but now we think it shows LLMs can learn effectively from a single example.",
    "title": "fast.ai &ndash; Can LLMs learn from a single example?",
    "auth": "Jeremy Howard and Jonathan Whitaker",
    "date": 0
  },
  {
    "url": "https://www.coursera.org/collections/llms-terms",
    "desc": "Large Language Models (LLMs) Definitions : A to Z Glossary Terms",
    "title": "Large Language Models (LLMs) Definitions : A to Z Glossary Terms",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://byronsalty.medium.com/my-growing-list-of-ai-and-llm-terminology-26d8b109a14f",
    "desc": "Industry Terms",
    "title": "Industry Terms",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://toloka.ai/blog/history-of-llms/",
    "desc": "The impressive speed at which AI has evolved has never been more apparent than it is now, with ChatGPT making headlines and the dramatic evolution of Large Language Models (LLMs) ever present in the media cycle. Millions of people worldwide have wasted no time adopting conversational AI tools in their day-to-day existence. These tools have not only enamored but also terrified audiences with their striking capabilities and efficiency and their potentially dangerous implications if not regulated w",
    "title": "The history, timeline, and future of LLMs",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://www.meetgrit.com/s/articles/a06Qp00000C09dtIAB/a-compact-guide-to-large-language-models-defining-llms-and-their-role-in-ai",
    "desc": "Grit Holdings",
    "title": "Grit Holdings",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://medium.com/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://medium.com/@msayef/llm-terminologies-decoded-a-beginners-quick-reference-guide-925f35794198",
    "desc": "Group 1",
    "title": "Group 1",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://medium.com/tales-of-tomorrow/a-quiet-revolution-mistral-ai-releases-sensational-new-ai-model-c17c663287f0",
    "desc": "a-quiet-revolution-mistral-ai-releases-sensational-new-ai-model-c17c663287f0",
    "title": "a-quiet-revolution-mistral-ai-releases-sensational-new-ai-model-c17c663287f0",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://towardsdatascience.com/gpt-3-rnns-and-all-that-deep-dive-into-language-modelling-7f67658ba0d5",
    "desc": "GPT-3, RNNs and All That: A Deep Dive into Language Modelling",
    "title": "GPT-3, RNNs and All That: A Deep Dive into Language Modelling",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://ai.plainenglish.io/fine-tuning-large-language-models-for-decision-support-a-comprehensive-guide-c1ef4c06abc6",
    "desc": "Fine-Tuning Large Language Models for Decision Support: A Comprehensive Guide",
    "title": "Fine-Tuning Large Language Models for Decision Support: A Comprehensive Guide",
    "auth": "unknown",
    "date": 0
  }
]